\documentclass{jsarticle}
\usepackage{amsmath, amssymb}
\begin{document}
\section*{Exercise 3.1 MLE for the Bernoulli/ binomial model}
\begin{align}
\frac{d}{d\theta}p(D|\theta) & = \frac{d}{d\theta}(\theta^{N_1}(1-\theta)^{N_0})\\
& = N_1 \theta^{N_1-1}(1-\theta)^{N_0}-N_0\theta^{N_1}(1-\theta)^{N_01}\\
& = \theta^{N_1-1}(1-\theta)^{N_0-1}(N_1(1-\theta)-N_0\theta) \\
& = \theta^{N_1-1}(1-\theta)^{N_0-1}(N_1-N\theta)\\
\therefore \theta_{\rm MLE} & =\frac{N_1}{N}
\end{align}

\section*{Exercise 3.2 Marginal likelihood for the Beta-Bernoulli model}
\begin{align}
p(D) & = \frac{[(\alpha_1)\cdots(\alpha_1+N_1-1)][(\alpha_0)\cdots(\alpha_0+N_0-1)]}{(\alpha)\cdots(\alpha + N - 1)}\\
& = \frac{[(\alpha_1)\cdots(\alpha_1+N_1-1)][(\alpha_0)\cdots(\alpha_0+N_0-1)]}{(\alpha_1+\alpha_0)\cdots(\alpha_1 + \alpha_0 + N - 1)}\\
& = \frac{\Gamma(\alpha_1+\alpha_0)}{\Gamma(\alpha_1+\alpha_0+N)}[(\alpha_1)\cdots (\alpha_1+N_1-1)][(\alpha_0)\cdots(\alpha_0+N_0-1)]\\
& = \frac{\Gamma(\alpha_1+\alpha_0)}{\Gamma(\alpha_1+\alpha_0+N)}\frac{\Gamma(\alpha_1+N_1)}{\Gamma(\alpha_1)}\frac{\Gamma(\alpha_0+N_0)}{\Gamma(\alpha_0)}\\
& = \frac{\Gamma(\alpha_1+N_1)\Gamma(\alpha_0+N_0)}{\Gamma(\alpha_1+\alpha_0+N)}\frac{\Gamma(\alpha_1+\alpha_0)}{\Gamma(\alpha_1)\Gamma(\alpha_0)}
\end{align}

\section*{Exercise 3.3 Posterior prdictive for Beta-Binomial model}
\begin{align}
Bb(1|\alpha_1',\alpha_0',1) & = \frac{B(1+\alpha_1',1-1+\alpha_0')}{B(\alpha_1',\alpha_0')} \begin{pmatrix} 1 \\ 1 \end{pmatrix} \\
& = \frac{B(1+\alpha_1', \alpha_0')}{B(\alpha_1',\alpha_0')}\\
& = \frac{\Gamma(\alpha_1'+\alpha_0')}{\Gamma(\alpha_1')\Gamma(\alpha_0')}\frac{\Gamma(1+\alpha_1')\Gamma(\alpha_0')}{\Gamma(1+\alpha_1'+\alpha_0')}\\
& = \frac{\Gamma(\alpha_1'+\alpha_0')}{\Gamma(\alpha_1')\Gamma(\alpha_0')}\frac{\alpha_1'\Gamma(\alpha_1')\Gamma(\alpha_0')}{(\alpha_1'+\alpha_0')\Gamma(\alpha_1'+\alpha_0')}\\
& = \frac{\alpha_1'}{\alpha_1'+\alpha_0'}
\end{align}

\section*{Exercise 3.4 Beta updating from censored likelihood}
\begin{align}
p(\theta,X<3) & = p(\theta)p(X<3|\theta)\\
& = p(\theta)(\sum_{k=0}^{2}p(X=k|\theta))\\
& = p(\theta)(\sum_{k=0}^{2}\theta^k(1-\theta)^{(5-k)})\\
& = Beta(\theta|1,1)(\sum_{k=0}^{2}\theta^k(1-\theta)^{(5-k)})\\
& = \sum_{k=0}^{2}\theta^k(1-\theta)^{(5-k)}
\end{align}

\section*{Exercise 3.5 Uninformative prior for log-odds ratio}
\begin{align}
p(\theta) & = p(\phi)|\frac{d\phi}{d\theta}|\\
& = p(\phi)\theta^{-1}(1-\theta)^{-1}\\
& \propto Beta(\theta|0,0)\ (\because p(\phi)\propto 1)
\end{align}

\section*{Exercise 3.6 MLE for the Poisson distribution}
\begin{align}
D & = \{x_1, x_2, \dots, x_N\}\\
p(D|\lambda) & = \Pi_{i=1}^{N}Poi(x_i|\lambda)\\
& = e^{-N\lambda}\frac{\lambda^{\sum_{i=1}^{N}x_i}}{\Pi_{i=1}^{N}(x_i!)}\\
\log p(D|\lambda) & = -N\lambda + \sum_{i=1}^{N}x_i\log \lambda - \sum_{i=1}^{N}\log x_i!\\
\frac{\partial}{\partial \lambda}\log p(D|\lambda) & = -N + \frac{1}{\lambda}\sum_{i=1}^{N}x_i\\
\therefore\lambda_{MLE} & =\frac{1}{N}\sum_{i=1}^{N}x_i
\end{align}

\section*{Exercise 3.7 Bayesian analysis of the Poisson distribution}
\subsection*{(a)}
\begin{align}
p(\lambda|D) & \propto p(\lambda)p(D|\lambda)\\
& \propto \lambda^{a-1}e^{-\lambda b}e^{-N\lambda}\frac{\lambda^{\sum_{i=1}^{N}x_i}}{\Pi_{i=1}^{N}(x_i!)}\\
& = e^{-\lambda(N+b)}\frac{\lambda^{\sum_{i=1}^{N}x_i}}{\Pi_{i=1}^{N}(x_i!)}\\
& \propto Ga(\lambda|a + \sum_{i=1}^{N}x_i, b + N)
\end{align}
\subsection*{(b)}
\begin{align}
\frac{a+\sum_{i=1}^{N}x_i}{b+N} & \rightarrow \frac{1}{N}\sum_{i=1}^{N}x_i\\
& = \lambda_{MLE}
\end{align}

\section*{Exercise 3.8 MLE for the uniform distribution}
\subsection*{(a)}
\begin{align}
D & = \{x_1, \dots, x_N\} \\
p(D|a) & = \Pi_{i=1}^{N}\frac{1}{2a}I(x_i\in [-a, a])
\end{align}
If $\forall i\ -a \leq x_i \leq a$, then $p(D|a)=\frac{1}{(2a)^n}$.
More smaller $a$, more larger $p(D|a)$.

$\hat{a}=\max \{|x_1|,\dots, |x_N| \}$

\subsection*{(b)}
\begin{align}
p(x_{n+1}|\hat{a}) & = \frac{1}{2\hat{a}}I(x_{n+1}\in[-\hat{a}, \hat{a}])\\
& = \begin{cases}
0 & (x_{n+1}\notin [-\hat{a}, \hat{a}])\\
\frac{1}{2\hat{a}} & (x_{n+1}\in [-\hat{a}, \hat{a}])
\end{cases} 
\end{align}

\subsection*{(c)}
If we use MLE approach, the probability between $-\hat{a}$ and $\hat{a}$ is 0.
Bayesian approach with introducing a wide range prior is better.

\end{document}
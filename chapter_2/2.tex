\documentclass{jsarticle}
\usepackage{amsmath, amssymb}
\begin{document}
\section*{Exercise 2.1 Probabilities are sensitive to the form of the question that was used to generate the answer}
\subsection*{(a)}
\begin{table}[htbp]
\centering
\begin{tabular}{cc}
C1 & C2 \\ \hline
B & B\\
B & G\\
G & B\\
G & G
\end{tabular}
\end{table}
If the neighbor has any boys, there is only a chance to the first of the three rows in the above table.
In this situation, the probability that one child is a girl is $\frac{2}{3}$.
\subsection*{(b)}
\begin{eqnarray*}
P(Run=C1)\cdot P(C2=G|Run=C1)+P(Run=C2)\cdot P(C1=G|Run=C2) & = & \frac{1}{2}\cdot\frac{1}{2}\\ & = & \frac{1}{2}
\end{eqnarray*}

\section*{Exercise 2.2 Legal reasoning}
\subsection*{(a)}
\begin{eqnarray}
G & = & \begin{cases}
1 & (\rm defendant\ is\ guilty) \\
0 & (\rm otherwise)
\end{cases}\\
B & = & \begin{cases}
1 & (\rm defendant's\ blood\ type\ matches\ one\ at\ the\ scene)\\
0 & (\rm otherwise)
\end{cases}\\
P(G=0|B=1) & = & \frac{P(B=1|G=0)\cdot P(G=0)}{P(B=1)}\\
& = & \frac{P(B=1|G=0)\cdot P(G=0)}{P(B=1|G=1)\cdot P(G=1)+P(B=1|G=0)\cdot P(G=0)}\\
& = & \frac{\frac{1}{1000}\times \frac{799999}{800000}}{1\times \frac{1}{800000} + \frac{1}{1000} \times \frac{799999}{800000}}\\
& \simeq & 0.999
\end{eqnarray}
The probability of the defendant is innocense given that defendant's blood type matches one at the scene is 0.999.
\subsection*{(b)}
The probabilities of guilty of 8000 people are all the same. This fact doesn't show that the defendant is innocense.

\section*{Exercise 2.3 Variance of a sum}
\begin{eqnarray}
V[X+Y] & = & E[\{(X+Y)-E[X+Y]\}^2]\\
& = & E[\{(X+Y)-(E[X]+E[Y])\}^2]\\
& = & E[\{(X-E[X])+(Y-E[Y])\}^2]\\
& = & E[(X-E[X])^2+(Y-E[Y])^2+2(X-E[X])(Y-E[Y])]\\
& = & E[(X-E[X])^2] + E[(Y-E[Y])^2] + 2E[(X-E[X])(Y-E[Y])]\\
& = & V[X] + V[Y] + 2{\rm cov}[X,Y]
\end{eqnarray}

\section*{Exercise 2.4 Bayes rule for medical diagnosis}
\begin{eqnarray}
X & = & \begin{cases}
1 & (\rm I\ have\ the\ disease) \\
0 & (\rm otherwise)
\end{cases}\\
Y & = & \begin{cases}
1 & (\rm testing\ positive)\\
0 & (\rm otherwise)
\end{cases}\\
P(X=1|Y=1) & = & \frac{P(Y=1|X=1)\cdot P(X=1)}{P(Y=1)}\\
& = & \frac{P(Y=1|X=1)\cdot P(X=1)}{P(Y=1|X=1)\cdot P(X=1)+P(Y=1|X=0)\cdot P(X=0)}\\
& = & \frac{\frac{99}{100}\times \frac{1}{10000}}{\frac{99}{100}\times \frac{1}{10000} + \frac{1}{100}\times \frac{9999}{10000}}\\
& = & \frac{99}{99 + 9999}\\
& = & \frac{1}{102}\\
& \simeq & 9.8\times 10^{-3}
\end{eqnarray}

\section*{Exercise 2.5 The Mohty Hall Problem}
\begin{eqnarray}
X & = & \begin{cases}
1  & (\rm the\ prize\ in\ door\ 1) \\
2  & (\rm the\ prize\ in\ door\ 2) \\
3  & (\rm the\ prize\ in\ door\ 3)
\end{cases}\\
Y & = & \begin{cases}
1  & (\rm the\ host\ opens\ door\ 1)\\
2  & (\rm the\ host\ opens\ door\ 2)\\
3  & (\rm the\ host\ opens\ door\ 3)
\end{cases}\\
P(X=1) & = & P(X=2) = P(X=3) = \frac{1}{3}\\
P(Y=3|X=1) & = & \frac{1}{2}\\
P(Y=3|X=2) & = & 1\\
P(Y=3|X=3) & = & 0\\
P(X=1|Y=3) & = & \frac{P(Y=3|X=1)\cdot P(X=1)}{P(Y=3)}\\
& = & \frac{P(Y=3|X=1)\cdot P(X=1)}{\sum_{i=1}^{3}P(Y=3|X=i)\cdot P(X=i)}\\
& = & \frac{\frac{1}{2}\times \frac{1}{3}}{\frac{1}{2}\times \frac{1}{3} + 1 \times \frac{1}{3}+0\times \frac{1}{3}}\\
& = & \frac{1}{3}\\
P(X=3|Y=3) & = & 0\ (\because P(Y=3|X=3)=0)\\
P(X=2|Y=3) & = & 1 - P(X=1|Y=3) - P(X=3|Y=3)\\
& = & \frac{2}{3}
\end{eqnarray}
This result shows the contestant should switch to door 2.

\section*{Exercise 2.6 Conditional Independence}
\subsection*{(a)}
\begin{align}
P(H,e_1,e_2) & = P(e_1,e_2|H)P(H)\\
P(H,e_1,e_2) & = P(H|e_1,e_2)p(e_1,e_2)\\
\therefore p(H|e_1,e_2) & =\frac{P(e_1,e_2|H)P(H)}{P(e_1,e_2)}
\end{align}
Above calculation shows (ii.) is sufficient. 
\subsection*{(b)}
\begin{align}
P(H|e_1,e_2) & = \frac{P(e_1,e_2|H)P(H)}{P(e_1,e_2)}\\
& = \frac{P(e_1|H)P(e_2|H)P(H)}{P(e_1,e_2)} (\because E_1\ {\rm and}\ E_2\ {\rm are\ conditionally\ independent\ given\ H}) \\
P(e_1,e_2) &= \sum_{i=1}^{K}P(e_1,e_2|H=i)P(H=i)\\
&= \sum_{i=1}^{K}P(e_1|H=i)P(e_2|H=i)P(H=i)
\end{align}
Above calculation shows (i.), (ii.) and (iii.) are sufficient.

\section*{Exercise 2.7 Pairwise independence does not imply mutual independence}
(Reference to http://www.cut-the-knot.org/Probability/MutuallyIndependentEvents.shtml)

There are four balls numbered as below.

\begin{align*}
B1 = 110 \\
B2 = 101 \\
B3 = 011 \\
B4 = 000
\end{align*}
For $k=1,2,3$ let $A_k$ be the event of drawing a ball with 1 in the $k$th position.
\begin{align}
P(A_1)=\frac{1}{2},\ P(A_2)=\frac{1}{2},\ P(A_3)=\frac{1}{2}\\
P(A_1,A_2)=\frac{1}{4},\ P(A_1, A_3)=\frac{1}{4},\ P(A_2, A_3)=\frac{1}{4}\\
\therefore P(A_1, A_2)=P(A_1)P(A_2)\\
P(A_1, A_3)=P(A_1)P(A_3)\\
P(A_2,A_3)=P(A_2)P(A_3)
\end{align}
This shows all pairs of variables are pairwise independence.
\begin{align}
P(A_1,A_2,A_3)=0\\
P(A_1)P(A_2)P(A_3)=\frac{1}{8}\\
\therefore P(A_1,A_2,A_3)=P(A_1)P(A_2)P(A_3)
\end{align}
This shows mutual independence doesn't hold.

\section*{Exercise 2.8 Conditional independence iff joint factorizes}
\subsection*{$(\Rightarrow)$}
\begin{align}
g(x,z)=p(x|z), h(y,z)=p(y|z)
\end{align}
\subsection*{$(\Leftarrow)$}
\begin{align}
p(x,y|z) & = g(x,z)h(y,z) \label{2.8}
\end{align}
We calculate the integral of left side and right side.
\begin{align}
\int_{x}p(x,y|z)dx & =\int_{x}g(x,z)h(y,z)dx\\
p(y|z) & = h(y,z)G(x,z) 
\end{align}
We calculate the integral of left side and right side.
\begin{align}
\int_{y}p(y|z)dy & =\int_{y}h(y,z)G(x,z)dy\\
H(y,z)G(x,z) & = 1
\end{align}
We calculate the integral of left side and right side of (\ref{2.8}).
\begin{align}
\int_{y}p(x,y|z)dy & = \int_{y}g(x,z)h(y,z)dy \\
p(x|z) & =g(x,z)H(y,z)
\end{align}
Finally, we get
\begin{align}
p(x,y|z) & = g(x,z)h(y,z)\\
& = \frac{p(x|z)}{H(y,z)}\frac{p(y|z)}{G(x,z)} \\
& = p(x|z)p(y|z)
\end{align}

\section*{Exercise 2.9 Conditional independence}
\subsection*{(a)}
\begin{align}
(X \bot W|Z,Y) & \Leftrightarrow p(X,W|Z,Y) = p(X|Z,Y)p(W|Z,Y) \label{2.9.a.1} \\
(X \bot Y|Z) & \Leftrightarrow p(X,Y|Z) = p(X|Z)p(Y|Z) \label{2.9.a.2} \\
(X\bot Y,W|Z) & \Leftrightarrow p(X,W|Z) = p(X|Z)p(W|Z) \label{2.9.a.3} \\
p(X,W,Z|Y) & = p(X,Y,Z)p(W|Z,Y)\ (\because (\ref{2.9.a.1})) \label{xyzw}\\
p(X,Y,Z) & = p(X,Z)p(Y|Z)\ (\because (\ref{2.9.a.2})) \label{xyz}
\end{align}
Because of (\ref{xyzw}) and (\ref{xyz}), we get
\begin{align}
p(X,Y,Z,W) & = p(X,Z)p(Y|Z)p(W|Z,Y)\\
& = p(X,Z)p(Y,W|Z)
\end{align}
We calculate the integral of left side and right side.
\begin{align}
\int_{Y}p(X,Y,Z,W)dY & = \int_{Y}p(X,Z)p(Y,W|Z)dY\\
p(X,Z,W) & = p(X,Z)p(W|Z)\\
\frac{p(X,W|Z)}{p(Z)} & = \frac{p(X|Z)}{p(Z)}p(W|Z)\\
p(X,W|Z) & = p(X|Z)p(W|Z)
\end{align}
We showed (\ref{2.9.a.1}) $\wedge$ (\ref{2.9.a.2}) $\Rightarrow$ (\ref{2.9.a.3}).
\subsection*{(b)}
Somebody help me!

\section*{Exercise 2.10 Deriving the inverse gamma density}
\begin{align}
p_y(y) & =p_x(x)|\frac{dx}{dy}|\\
 & = \frac{b^a}{\Gamma(a)}x^{a-1}e^{-xb}|-\frac{1}{y^2}|\\
 & = \frac{b^a}{\Gamma(a)}y^{-(a-1)}e^{-\frac{b}{y}}\frac{1}{y^2}\\
 & = \frac{b^a}{\Gamma(a)}y^{-(a+1)}e^{-\frac{b}{y}}\\
 & = IG(y|a,b)
\end{align}

\section*{Exercise 2.11 Normalization constant for a 1D Gaussian}
\begin{align}
Z^2 & = \int_{0}^{2\pi}\int_{0}^{\infty}r\exp(-\frac{r^2}{2\sigma^2})drd\theta\\
& = \int_{0}^{2\pi}d\theta\int_{0}^{\infty}r\exp(-\frac{r^2}{2\sigma^2})dr\\
& = 2\pi \int_{0}^{\infty}r\exp(-\frac{r^2}{2\sigma^2})dr\\
& = 2\pi \left[-\sigma^2\exp(-\frac{r^2}{2\sigma^2})\right]_{0}^{\infty}\\
& = 2\pi\sigma^2\\
\therefore Z & = \sqrt{2\pi \sigma^2}
\end{align}

\section*{Exercise 2.12 Expressing mutual information in terms of entropies}
\begin{align}
I(X,Y) & = \sum_x\sum_yp(x,y)\log\frac{p(x,y)}{p(x)p(y)}\\
& = \sum_x\sum_yp(x,y)\log\frac{p(x|y)p(y)}{p(x)p(y)}\\
& = \sum_x\sum_yp(x,y)(-\log p(x)+\log p(x|y))\\
& = -\sum_x\sum_yp(x,y)\log p(x) + \sum_x\sum_yp(x,y)\log p(x,y)\\
& = -\sum_xp(x)\log p(x) + \sum_x\sum_yp(x,y)\log p(x|y)\\
& = H(x) + \sum_x\sum_yp(x,y)\log p(x|y)\\
& = H(X) + \sum_x\sum_yp(y)p(x|y)\log p(x|y)\\
& = H(X) + \sum_y p(y)\sum_xp(x|y)\log p(x|y)\\
& = H(X) - \sum_y p(y) H(X|Y=y)\\
& = H(X) - H(X|Y)
\end{align}
$I(X,Y)=H(Y)-H(X|Y)$ can be shown like as below.

\section*{Exercise 2.13 Mutual information for correlated normals}
\begin{align}
I(X,Y) & = -H(X) + H(X_1) + H(X_2)\\
& = -\frac{1}{2}\log_2 [4\pi^2 e^2 \sigma^4 (1-\rho^2)] + \frac{1}{2}\log_2 [2 \pi e \sigma^2] + \frac{1}{2}\log_2 [2\pi e \sigma^2 ]\\
& = \frac{1}{2}\log_2{\frac{4\pi^2e^2\sigma^4}{4\pi^2e^2\sigma^4(1-\rho^2)}}\\
& = \frac{1}{2}\log_2 \frac{1}{1 - \rho^2}\\
& = -\frac{1}{2}\log_2(1-\rho^2)
\end{align}
and we get 
\begin{align}
\begin{cases}
\rho=1 : I(X_1,X_2)=\infty & (\rm perfect\ correlation)\\
\rho=0 : I(X_1, X_2)=0 & (\rm independent)\\
\rho=-1 : I(X_1, X_2)=\infty & (\rm perfect\ correlation)
\end{cases}
\end{align}

\section*{Exercise 2.14 A measure of correlation (normalized mutual information)}
\subsection*{(a)}
\begin{align}
r & = 1 - \frac{H(Y|X)}{H(X)}\\
& = \frac{H(X)-H(Y|X)}{H(X)}\\
& = \frac{H(Y)-H(Y|X)}{H(X)}\\
& = \frac{I(X,Y)}{H(X)}
\end{align}
\subsection*{(b)}
$(0 \leq \frac{I(X,Y)}{H(X)})$
\begin{align}
I(X,Y)\geq 0 \land H(X) \geq 0\\
\therefore \frac{I(X,Y)}{H(X)} \geq 0
\end{align}
$\frac{I(X,Y)}{H(X)\leq 1}$
\begin{align}
H(X)-I(X,Y) & = H(X) - (H(X)-H(X|Y)\\
& = H(X|Y)\\
& \geq 0\\
\therefore \frac{I(X,Y)}{H(X)} \leq 1
\end{align}
\subsection*{(c)}
If $r=0$, then $I(X,Y)=0$.
And we get $p(X,Y)=p(X)p(Y)$.
This means $X$ and $Y$ are independent.
\subsection*{(d)}
If $r=1$, then $I(X,Y)=H(X)$.
And we get $H(X|Y)=0$.
This means perfect correlation.

\section*{Exercise 2.15 MLE minimizes KL divergence th the empirical distribution}
\begin{align}
\arg \min_\theta KL(p_{emp}||q(x;\theta)) & = \arg \min_\theta \sum_{i=1}^{N}p_{emp}(x_i)\log \frac{p_{emp}(x_i)}{q(x_i;\theta)}\\
& =  \arg \min_\theta \{-H(p_{emp})-\sum_{i=1}^{N}p_{emp}(x_i)\log q(x_i;\theta) \}\\
& = \arg \max_\theta \sum_{i=1}^{N}p_{emp}(x_i)\log q(x_i;\theta)\\
& = \arg \max_\theta \frac{1}{N}\sum_{i=1}^{N}\log q(x_i;\theta)\\
& = \hat{\theta}_{MLE}
\end{align}

\section*{Exercise 2.16 Mean, mode, variance for the beta distribution}
\subsection*{mean}
\begin{align}
E[\theta] & = \int \theta \frac{1}{B(a,b)}\theta^{a-1}(1-\theta)^{b-1}d\theta\\
& = \int \frac{1}{B(a,b)}\theta^{a}(1-\theta)^{b-1}d\theta\\
& = \int \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\theta^{a}(1-\theta)^{b-1}d\theta\\
& = \int \frac{\Gamma(a+1+b)}{\Gamma(a+1)\Gamma(b)}\frac{a}{a+b}\theta^{a}(1-\theta)^{b-1}d\theta\\
& = \frac{a}{a+b}\int \frac{1}{B(a+1,b)}\theta^{a}(1-\theta)^{b-1}d\theta\\
& = \frac{a}{a+b}
\end{align}
\subsection*{mode}
\begin{align}
\frac{d}{d\theta}Beta(\theta|a,b) & = \frac{a-1}{B(a,b)}\theta^{a-2}(1-\theta)^{b-1}-\frac{b-1}{B(a,b)}\theta^{a-1}(1-\theta)^{b-2}\\
& = \frac{\theta^{a-2}(1-\theta)^{b-2}}{B(a,b)}\{(a-1)(1-\theta) - (b-1)\theta \} \\
& = \frac{\theta^{a-2}(1-\theta)^{b-2}}{B(a,b)}\{-(a + b - 2)\theta + a - 1 \}\\
\theta_{mode} & = \frac{a-1}{a+b-2}
\end{align}
\subsection*{variance}
\begin{align}
E[\theta^2] & = \int \theta^2\frac{1}{B(a,b)}\theta^{a-1}(1-\theta)^{b-1}d\theta\\
& = \int-\theta(1-\theta)\frac{1}{B(a,b)}\theta^{a-1}(1-\theta)^{b-1}d\theta + \int \theta \frac{1}{B(a,b)}\theta^{a-1}(1-\theta)^{b-1}d\theta\\
& = -\int\frac{1}{B(a,b)}\theta^a(1-\theta)^bd\theta + \frac{a}{a+b}\\
& = -\frac{ab}{(a+b+1)(a+b)}\int\frac{1}{B(a+1,b+1)}\theta^a(1-\theta)^bd\theta + \frac{a}{a+b}\\
& = -\frac{ab}{(a+b+1)(a+b)} + \frac{a}{a+b}\\
& = \frac{a^2+a}{(a+b+1)(a+b)}\\
V[\theta] & = E[\theta^2] - E[\theta]^2 \\
& = \frac{ab}{(a+b)^2(a+b+1)}
\end{align}

\section*{Exercise 2.17 Expected value of the minimum}
\begin{align}
Z & = \min(X,Y)\\
F(t) & = p(Z \leq t)\\
 & = 1 - p(Z \ge t)\\
 & = 1 - p(X \ge t, Y \ge t)\\
 & = 1 - p(X \ge t)p(Y \ge t)\\
 & = 1 - (1 - t)^2\\
 & = -t^2+2t \\
f(t) & = \frac{d}{dt}F(t)\\
 & = -2t + 2\\
\int_{0}^{1}tf(t)dt & = \int_{0}^{1}(-2t^2+2t)dt\\
 & = [-\frac{2}{3}t^3+t^2]_0^1\\
 & = \frac{1}{3}
\end{align}
\end{document}